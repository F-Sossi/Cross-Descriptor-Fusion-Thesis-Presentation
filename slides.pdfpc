{"pdfpcFormat":2,"disableMarkdown":true,"noteFontSize":20,"pages":[{"idx":0,"label":"1","overlay":0,"note":"TITLE SLIDE\n- Welcome everyone, introduce yourself\n- Thank the committee: Prof. Olson (chair), Prof. Chen, Prof. Si\n- \"Today I'll present my thesis on cross-detector descriptor fusion for local feature matching\"\n\n"},{"idx":1,"label":"2","overlay":0,"note":"MIOS - read this slowly, it's the one-sentence summary.\n\"Selecting high-quality keypoints through detector consensus and scale filtering, then fusing complementary descriptors with proper magnitude matching, yields large improvements.\"\n\n"},{"idx":2,"label":"3","overlay":0,"note":"OUTLINE\n- Four-part structure: Framing, Process, Results, Lessons Learned\n- ~35-40 minutes, then questions\n- Briefly mention each section\n\n"},{"idx":3,"label":"4","overlay":0,"note":"THE PROBLEM\n- Local features are the backbone of many CV systems\n- Detection finds WHERE, description encodes WHAT\n- Different families have different strengths - that's our opportunity\n- Key question: can we combine them systematically?\n\n"},{"idx":4,"label":"5","overlay":0,"note":"STATE OF THE ART\n- Left side: what already existed when I started\n- Right side: the gaps - no systematic fusion study, no understanding of when it helps/hurts, no color patch benchmark\n- This is where our work fits\n\n"},{"idx":5,"label":"6","overlay":0,"note":"KEY CONCEPTS\n- Walk through the pipeline diagram left to right\n- Define keypoint, descriptor, mAP for the audience\n- Make sure everyone understands these before diving into results\n- Bojanic et al. metrics are our evaluation standard\n\n"},{"idx":6,"label":"7","overlay":0,"note":"RESEARCH GOALS\n- Four research questions, each with a concrete success criterion\n- RQ1: detector consensus - measured by intersection vs full set\n- RQ2: color fusion - measured on our new color benchmark\n- RQ3: compatibility patterns - when does fusion help vs hurt\n- RQ4: scale impact - quantify the relationship\n- All evaluated on HPatches: 116 sequences\n\n"},{"idx":7,"label":"8","overlay":0,"note":"BENEFITS & BENEFICIARIES\n- Research community: fusion rules, keypoint quality evidence, new benchmark, V/M framework\n- Practitioners: concrete recipes, free performance boost from scale filtering, open-source tool\n- Applicable to real systems: SLAM, SfM, visual localization\n\n"},{"idx":8,"label":"9","overlay":0,"note":"KEY DECISION: TWO PIPELINES\n- This was a critical design decision\n- Full-image pipeline: tests detector AND descriptor together - can't separate effects\n- Patch pipeline: holds keypoint quality constant - isolates descriptor effects\n- Without two pipelines, we'd confound our variables\n\n"},{"idx":9,"label":"10","overlay":0,"note":"ARCHITECTURE\n- C++ framework, 10 descriptors, YAML-driven\n- SQLite database tracks all 100+ experiments\n- Three metrics from Bojanic et al.\n- Open source on GitHub\n\n"},{"idx":10,"label":"11","overlay":0,"note":"COLOR PATCH BENCHMARK\n- Original HPatches only has grayscale patches\n- We NEEDED color for HoNC and RGBSIFT evaluation\n- Re-extracted from original images using stored keypoints + homographies\n- Validated: SIFT baseline 22.9% vs 25.47% original\n- Without this, we couldn't answer RQ2 at all\n\n"},{"idx":11,"label":"12","overlay":0,"note":"SPATIAL INTERSECTION\n- Walk through the 5 steps\n- Point to the diagram: blue=SIFT, red=KeyNet, green circles=matches\n- Intuition: if two VERY different detectors agree, it's a real feature\n- This is a quality filter, not just a count reducer\n\n"},{"idx":12,"label":"13","overlay":0,"note":"SCALE CHARACTERISTICS\n- SIFT: tiny keypoints, average 4.45 pixels\n- KeyNet: much larger, average 49.83 pixels\n- Key insight: bigger keypoint = bigger patch = more information\n- 4px samples 16x16, 10px samples 40x40 - huge difference\n\n"},{"idx":13,"label":"14","overlay":0,"note":"SURPRISE #1: SAME-FAMILY FUSION FAILS\n- We expected SIFT + RGBSIFT to add color info\n- ACTUALLY: fusion was WORSE than the best individual\n- Why? They capture correlated gradient information\n- Even color channels don't add enough new signal\n- First lesson: not all fusion helps\n\n"},{"idx":14,"label":"15","overlay":0,"note":"SURPRISE #2: CROSS-FAMILY BREAKS\n- Expected SIFT + HardNet to be great\n- It was WORSE than either alone\n- Root cause: magnitude mismatch\n- SIFT values go up to 512, HardNet stays around 0.3\n- In L2 distance, SIFT completely dominates - HardNet is invisible\n- This was the key debugging moment of the thesis\n\n"},{"idx":15,"label":"16","overlay":0,"note":"ADJUSTMENT: NORMALIZATION\n- Solution: L2-normalize each component BEFORE fusion\n- Show the equation\n- Without: failed. With: 46.0% mAP\n- This led us to make it a configurable option in the framework\n- Important engineering lesson: the fix was simple once we found the root cause\n\n"},{"idx":16,"label":"17","overlay":0,"note":"SURPRISE #3: KEYPOINT QUALITY\n- We started focused on descriptors\n- Discovered keypoint selection matters just as much\n- Table shows: switching SIFT to HardNet = +20%, scale filtering SIFT = +18%, intersection on HardNet = +18%\n- These are comparable gains! Keypoint quality is underappreciated\n\n"},{"idx":17,"label":"18","overlay":0,"note":"BASELINE PERFORMANCE\n- Starting point for all comparisons\n- SIFT ~44%, RootSIFT ~47%, HardNet/SOSNet ~64%\n- CNN descriptors ~20% better than traditional\n- SIFT prefers viewpoint, CNN prefers illumination\n- All improvements in next slides measured from these baselines\n\n"},{"idx":18,"label":"19","overlay":0,"note":"SCALE CONTROL IMPACT\n- Point to the figure\n- SIFT: 44.5% to 62.8% = +18.3% absolute\n- HardNet: 64.5% to 78.1% = +13.6% absolute\n- Just filtering to top 25% by scale!\n- Quality over quantity - fewer but better keypoints\n- Answers RQ4: scale has large, consistent impact\n\n"},{"idx":19,"label":"20","overlay":0,"note":"INTERSECTION PROGRESSION\n- Three stages shown in the figure\n- Full set: 64.5% -> Scale-controlled: 78.1% -> Intersection: 82.1%\n- Each stage adds cumulative improvement\n- Intersection adds +4% BEYOND scale control\n- Answers RQ1: yes, detector consensus is a quality signal\n\n"},{"idx":20,"label":"21","overlay":0,"note":"CNN + CNN FUSION\n- On intersection keypoints\n- HardNet alone: 82.1%, SOSNet alone: 82.0%\n- Concatenation: 93.4% - that's +11.3%!\n- Averaging: 92.3% - good but concatenation is better\n- They learn complementary representations despite similar training\n\n"},{"idx":21,"label":"22","overlay":0,"note":"DISCRIMINATOR-MATCHER FRAMEWORK\n- V/M ratio = verification / matching performance\n- HoNC: 3.84x - great at rejecting false matches (discriminator)\n- HardNet: 1.84x - trained for correspondence (matcher)\n- This framework PREDICTS fusion outcomes\n- Pairing discriminator + matcher should be best\n\n"},{"idx":22,"label":"23","overlay":0,"note":"PATCH BENCHMARK FUSION\n- Point to the figure\n- Best: HoNC + SOSNet = 50.6% (discriminator + matcher)\n- CNN + CNN = 49.9% (complementary learned features)\n- SIFT + HoNC = 15.5% - WORSE than either alone (two weak matchers)\n- Answers RQ2 and RQ3: color adds value with strong matcher, complementarity is key\n\n"},{"idx":23,"label":"24","overlay":0,"note":"ANSWERING RESEARCH QUESTIONS\n- Go through each RQ with its answer\n- RQ1: YES - intersection +18%\n- RQ2: YES - HoNC+SOSNet beats SOSNet alone\n- RQ3: complementarity determines success, magnitude matching required\n- RQ4: scale is dominant - comparable to switching descriptor families\n- All four questions answered with quantitative evidence\n\n"},{"idx":24,"label":"25","overlay":0,"note":"TECHNICAL INSIGHTS\n- Three key lessons\n- 1. Keypoint quality is underappreciated in the literature\n- 2. Investigating failures led to the magnitude matching discovery\n- 3. Two pipelines prevented false conclusions about fusion\n\n"},{"idx":25,"label":"26","overlay":0,"note":"ENGINEERING & PROCESS\n- 1. Experiment infrastructure (SQLite + YAML) was worth the investment\n- 2. Reproducibility requires tooling - every result from one YAML file\n- 3. The V/M ratio framework emerged from data visualization, not theory\n\n"},{"idx":26,"label":"27","overlay":0,"note":"FUTURE WORK\n- Short-term: learned weights, tolerance sensitivity, more descriptors\n- Long-term: end-to-end learning, other datasets, real-time deployment\n- Plenty of directions to extend this work\n\n"},{"idx":27,"label":"28","overlay":0,"note":"LIMITATIONS\n- Single dataset (HPatches) - may not generalize\n- Concatenation doubles dimensionality (but filtering reduces total cost 28x)\n- KeyNet dependency - might not transfer to other detectors\n- Fixed equal weighting - learned weights could help\n\n"},{"idx":28,"label":"29","overlay":0,"note":"SUMMARY OF CONTRIBUTIONS\n- Read through the six contributions\n- Emphasize the bottom-line message: keypoint selection matters as much as descriptor choice\n- This is the take-home message\n\n"},{"idx":29,"label":"30","overlay":0,"note":"THANK YOU\n- Thank the committee again\n- Thank the audience\n- Open for questions\n- GitHub link available for anyone interested"}]}